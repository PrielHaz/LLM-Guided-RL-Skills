# Interpretable Reinforcement Learning via LLM-Guided Skills

This project explores the integration of Large Language Models (LLMs) with Hierarchical Reinforcement Learning (HRL) to create interpretable and semantically meaningful skills for agents. The framework generates high-level skills from agent trajectories, allowing the agent to perform complex tasks by selecting either primitive actions or LLM-generated skills.

---

## Setup

### 1. Define your deepseek API Key

```bash
export DEEPSEEK_API_KEY="your_api_key_here"
```

### 2. Install Dependencies

Ensure you have the required packages installed:

```bash
pip install crafter
```

Other necessary dependencies are standard RL and Python packages.

---

## Generating Skills Descriptions

### Running the skill generation process

Navigate to the `skills_gen` directory from the project root:

```bash
cd skills_gen
```

#### 1. Generate skills from a primitive agent trained for 1M steps:

```bash
python skills_generator.py
```

#### 2. Generate skills from a randomly initialized agent:

```bash
python skills_generator.py --from_non_trained
```

#### 3. Customizing skill generation

Modify the following parameters in `skills_generator.py`:

- `num_skills`: Defines the number of skills to generate.
- `traj_len`: Controls the length of each trajectory parsed by the LLM to generate one skill.

### Generated Directories

Running the script will create the following directories inside the skills_gen directory:

1. **captioned_trajectories/**: Stores agent trajectories in text format (captioned trajectories) along with videos that include frame-by-frame states and annotations of actions taken.
2. **context_for_skill_generation/**: Contains a context file for each generated skill, which is used by the LLM to create the corresponding skill description.
3. **option_policies_descriptions/**: Stores skill text descriptions (option policies) generated by the LLM.

---

## Training the Agent

### Running the Training Script

Move to the project root and update `main.ipynb` with the following:

- Set `use_skills = True` to use skills, or `False` to train without skills.
- If `use_skills` is set to `True`, specify the directory containing the skill descriptions. If you ran `skills_generator.py` and kept the default output location, you can use:
  ```python
  skills_descriptions_dir = "./skills_gen/option_policies_descriptions"
  ```
- Use the default training parameters or customize the following parameters in `main.ipynb`:

  - `steps`: Number of training steps.
  - `save_freq`: Save checkpoint every `save_freq` steps.
  - `eval_freq`: Evaluate model every `eval_freq` steps.
  - `n_eval_episodes`: Defines the number of episodes evaluated at each evaluation interval (`eval_freq` steps).

- Run all cells in `main.ipynb` to start training.

### Results

- A `results/` directory will be created containing:
  - **Training and evaluation figures** (mean reward, episode length, etc.).
  - **Model checkpoints**.
  - **Logs and additional training metadata**.

### Monitoring Training with TensorBoard

- In `main.ipynb`, use the TensorBoard cell's output link to view training/evaluation graphs.
- You can also start TensorBoard manually to compare experiments by running:
  ```bash
  tensorboard --logdir=results/
  ```
