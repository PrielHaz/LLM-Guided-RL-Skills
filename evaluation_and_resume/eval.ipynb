{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't use this notebook, to be implemented in the future.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# add to path the father directory\n",
    "try:  # if py file\n",
    "    sys.path.append(os.path.abspath(os.path.dirname(os.path.dirname(__file__))))\n",
    "except:  # if ipynb file will run this\n",
    "    sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from typing import List\n",
    "import crafter\n",
    "import stable_baselines3\n",
    "from stable_baselines3.common.callbacks import (\n",
    "    CheckpointCallback,\n",
    "    EvalCallback,\n",
    "    CallbackList,\n",
    ")\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import plot_results\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from crafter_module.crafter_gymnasium import CrafterGymnasium\n",
    "from utils.llm_skills_args import Skill\n",
    "from utils import util_funcs, constants\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from nlp_module.option_policy import (\n",
    "    OptionPolicy,\n",
    "    OptionPolicyResponse,\n",
    "    DeepseekClassifier,\n",
    ")\n",
    "from utils.llm_skills_args import LLM_Skills_Args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncomment the option you want, with\\without skills\n",
    "\n",
    "### Primitive actions only:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primitive example:\n",
    "model_path = \"../results/crafter_primitive_ppo/primitive_1M/final_model.zip\"\n",
    "env_kwargs = {\n",
    "    \"raw_env_kwargs\": constants.DEFAULT_RAW_ENV_KWARGS,\n",
    "    \"verbose\": 0,\n",
    "    \"print_file_path\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With skills example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "model_path = \"../results/crafter_primitive_ppo/exp0_skills/final_model.zip\"\n",
    "exp_dir = os.path.dirname(model_path)\n",
    "logs_dir = os.path.join(exp_dir, \"logs\")\n",
    "print_file_path = os.path.join(logs_dir, \"external_eval_prints.txt\")\n",
    "mask_actions_indices = [0]\n",
    "\n",
    "skills_descriptions_dir = \"../skills_gen/option_policies_descriptions\"\n",
    "skills: List[Skill] = util_funcs.load_skills(skills_descriptions_dir)\n",
    "for i, skill in enumerate(skills):\n",
    "    print(f\"{i}: {skill}\")\n",
    "\n",
    "verbose = 1\n",
    "option_policy = DeepseekClassifier(\n",
    "    model=\"deepseek-chat\", verbose=verbose, print_file_path=print_file_path\n",
    ")\n",
    "num_steps_pass_llm = 0\n",
    "default_action_index = 5\n",
    "llm_skills_args = LLM_Skills_Args(\n",
    "    skills=skills,\n",
    "    option_policy=option_policy,\n",
    "    num_steps_pass_llm=num_steps_pass_llm,\n",
    "    default_action_index=default_action_index,\n",
    ")\n",
    "env_kwargs = {\n",
    "    \"raw_env_kwargs\": constants.DEFAULT_RAW_ENV_KWARGS,\n",
    "    \"llm_skills_args\": llm_skills_args,\n",
    "    \"verbose\": verbose,\n",
    "    \"print_file_path\": print_file_path,\n",
    "    \"mask_actions_indices\": mask_actions_indices,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CrafterGymnasium(\n",
    "    **env_kwargs,\n",
    ")\n",
    "\n",
    "model = PPO.load(model_path, env=env)\n",
    "print(f\"Model trained for {model.num_timesteps} timesteps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncomment the evalution method you want to perform:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the agent using sb3 func that is not parallel and returns only mean and std:\n",
    "# mean_reward, std_reward = evaluate_policy(\n",
    "#     model, env, n_eval_episodes=10, deterministic=True\n",
    "# )\n",
    "# print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalent to their implementation, its not paralel!\n",
    "def eval_episodes(model, env, num_episodes=5):\n",
    "    rewards = []\n",
    "    episodes_lengths = []\n",
    "    for ep_idx in range(num_episodes):\n",
    "        print(f\"\\n\\n\\n @@@@@ Evaluating episode: {ep_idx} @@@@@ \\n\\n\")\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        step_idx = 0\n",
    "        while not done:\n",
    "            print(f\"Step: {step_idx} of episode {ep_idx}\")\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "            step_idx += 1\n",
    "        rewards.append(episode_reward)\n",
    "        episodes_lengths.append(step_idx)\n",
    "    return rewards, episodes_lengths\n",
    "\n",
    "\n",
    "rewards, episodes_lengths = eval_episodes(model, env, num_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rewards)\n",
    "print(f\"Mean reward: {np.mean(rewards):.2f} +/- {np.std(rewards):.2f}\")\n",
    "print(\n",
    "    f\"Mean episode length: {np.mean(episodes_lengths):.2f} +/- {np.std(episodes_lengths):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_episodes_parallel(\n",
    "    model, env_ctor, num_episodes_eval_in_parallel=5, eval_env_parallel_kwargs=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluates the model on multiple episodes in parallel.\n",
    "\n",
    "    :param model: Trained RL model\n",
    "    :param env_ctor: Constructor for the environment (e.g., Dog, not an instance)\n",
    "    :param num_episodes_eval_in_parallel: Number of evaluation episodes\n",
    "    :param eval_env_parallel_kwargs: Additional kwargs for creating evaluation environments\n",
    "    :return: List of episode rewards\n",
    "    \"\"\"\n",
    "    if eval_env_parallel_kwargs is None:\n",
    "        eval_env_parallel_kwargs = {}\n",
    "\n",
    "    # Create parallel environments\n",
    "    def make_env():\n",
    "        return env_ctor(**eval_env_parallel_kwargs)\n",
    "\n",
    "    envs = SubprocVecEnv([make_env for _ in range(num_episodes_eval_in_parallel)])\n",
    "\n",
    "    obs, info = envs.reset()\n",
    "    dones = np.array([False] * num_episodes_eval_in_parallel)\n",
    "    episode_rewards = np.zeros(num_episodes_eval_in_parallel)\n",
    "\n",
    "    while not np.all(dones):\n",
    "        actions, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, truncated, info = envs.step(actions)\n",
    "        episode_rewards += rewards * (~dones)  # Only add reward for ongoing episodes\n",
    "\n",
    "    envs.close()\n",
    "    return episode_rewards.tolist()\n",
    "\n",
    "\n",
    "rewards = eval_episodes_parallel(\n",
    "    model,\n",
    "    CrafterGymnasium,\n",
    "    num_episodes_eval_in_parallel=10,\n",
    "    eval_env_parallel_kwargs=env_kwargs,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UbuntuDec24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
