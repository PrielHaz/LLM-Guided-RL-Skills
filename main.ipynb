{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_num: 53\n"
     ]
    }
   ],
   "source": [
    "# read exp_num.txt, if not exist, create it with 0.\n",
    "# otherwise, read the number and increase it by 1.\n",
    "exp_file_path = \"./utils/exp_num.txt\"\n",
    "try:\n",
    "    with open(exp_file_path, \"r\") as f:\n",
    "        exp_num = int(f.read()) + 1\n",
    "    with open(exp_file_path, \"w\") as f:\n",
    "        f.write(str(exp_num))\n",
    "except:\n",
    "    with open(exp_file_path, \"w\") as f:\n",
    "        exp_num = 0\n",
    "        f.write(str(exp_num))\n",
    "\n",
    "# Manually for resume training(then comment it):\n",
    "# exp_num = 44\n",
    "print(f\"exp_num: {exp_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Skill(name=Resource Gathering, description=Option skill 'Resource Gathering'. This policy directs the agent to prioritize collecting essential ...)\n",
      "1: Skill(name=Basic Survival Strategy, description=Option skill 'Basic Survival Strategy'. This policy directs the agent to prioritize essential surviv...)\n",
      "2: Skill(name=Combat Strategy and Enemy Management, description=Option skill 'Combat Strategy and Enemy Management'. This policy directs the agent to handle hostile...)\n",
      "3: Skill(name=Efficient Crafting and Tool Progression, description=Option skill 'Efficient Crafting and Tool Progression'. This policy directs the agent to prioritize ...)\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import crafter\n",
    "import stable_baselines3\n",
    "from stable_baselines3.common.callbacks import (\n",
    "    CheckpointCallback,\n",
    "    EvalCallback,\n",
    "    CallbackList,\n",
    ")\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import plot_results\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from crafter_module.crafter_gymnasium import CrafterGymnasium\n",
    "from utils import util_funcs\n",
    "from utils.llm_skills_args import Skill\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from nlp_module.option_policy import (\n",
    "    OptionPolicy,\n",
    "    OptionPolicyResponse,\n",
    "    DeepseekClassifier,\n",
    ")\n",
    "from utils import constants\n",
    "from utils.llm_skills_args import LLM_Skills_Args\n",
    "import pickle\n",
    "\n",
    "\n",
    "verbose = 1\n",
    "# use_skills = True\n",
    "use_skills = False\n",
    "\n",
    "env_results_dir = \"./results/crafter/\"\n",
    "if use_skills:\n",
    "    exp_dir = os.path.join(env_results_dir, \"with_skills\", f\"exp{exp_num}\")\n",
    "else:\n",
    "    exp_dir = os.path.join(env_results_dir, \"primitive\", f\"exp{exp_num}\")\n",
    "checkpoints_dir = os.path.join(exp_dir, \"checkpoints\")\n",
    "logs_dir = os.path.join(exp_dir, \"logs\")\n",
    "eval_dir = os.path.join(exp_dir, \"eval\")\n",
    "monitor_dir = os.path.join(exp_dir, \"monitor\")\n",
    "figures_dir = os.path.join(exp_dir, \"figures\")\n",
    "\n",
    "os.makedirs(env_results_dir, exist_ok=True)\n",
    "os.makedirs(exp_dir, exist_ok=True)\n",
    "os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "os.makedirs(eval_dir, exist_ok=True)\n",
    "os.makedirs(monitor_dir, exist_ok=True)\n",
    "os.makedirs(figures_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_file_path = os.path.join(logs_dir, \"prints.txt\")\n",
    "eval_print_file_path = os.path.join(logs_dir, \"eval_prints.txt\")\n",
    "\n",
    "if use_skills:\n",
    "    # Take skills proposed by LLM looking at trajectories of trained(1M steps) primitive agent:\n",
    "    skills_descriptions_dir = \"./skills_gen/skills_created_sessions/deepseekV3/from_1M_trained_primitive/option_policies_descriptions\"\n",
    "    # Take skills proposed by looking at trajectories of non-trained agent:\n",
    "    # skills_descriptions_dir = \"./skills_gen/skills_created_sessions/deepseekV3/from_non_trained_agent/option_policies_descriptions\"\n",
    "\n",
    "    # skills_descriptions_dir = \"./skills_gen/option_policies_descriptions\"\n",
    "    skills: List[Skill] = util_funcs.load_skills(skills_descriptions_dir)\n",
    "    for i, skill in enumerate(skills):\n",
    "        print(f\"{i}: {skill}\")\n",
    "    # option_policy = GeminiClassifier(verbose=verbose)\n",
    "    option_policy = DeepseekClassifier(\n",
    "        model=\"deepseek-chat\", verbose=verbose, print_file_path=print_file_path\n",
    "    )\n",
    "\n",
    "    num_steps_pass_llm = 0\n",
    "    default_action_index = 5\n",
    "    llm_skills_args = LLM_Skills_Args(\n",
    "        skills=skills,\n",
    "        option_policy=option_policy,\n",
    "        num_steps_pass_llm=num_steps_pass_llm,\n",
    "        default_action_index=default_action_index,\n",
    "    )\n",
    "    mask_actions_indices = [0]  # noop\n",
    "\n",
    "    # contain raw_env_kwargs and the other ones.\n",
    "    env_kwargs = {\n",
    "        \"raw_env_kwargs\": constants.DEFAULT_RAW_ENV_KWARGS,\n",
    "        \"llm_skills_args\": llm_skills_args,\n",
    "        \"verbose\": verbose,\n",
    "        \"print_file_path\": print_file_path,\n",
    "        \"mask_actions_indices\": mask_actions_indices,\n",
    "    }\n",
    "    # eval_env_kwargs is to get env kwargs and add the eval_print_file_path:\n",
    "    eval_env_kwargs = {**env_kwargs, \"print_file_path\": eval_print_file_path}\n",
    "else:\n",
    "    # For primitive training without skills:\n",
    "    env_kwargs = {\n",
    "        \"raw_env_kwargs\": constants.DEFAULT_RAW_ENV_KWARGS,\n",
    "        \"verbose\": verbose,\n",
    "        \"print_file_path\": print_file_path,\n",
    "    }\n",
    "    eval_env_kwargs = {**env_kwargs, \"print_file_path\": eval_print_file_path}\n",
    "\n",
    "env = CrafterGymnasium(**env_kwargs)\n",
    "eval_env = CrafterGymnasium(**eval_env_kwargs)\n",
    "\n",
    "train_monitor_filename = os.path.join(monitor_dir, \"train_monitor.csv\")\n",
    "env = Monitor(env, filename=train_monitor_filename)\n",
    "eval_monitor_filename = os.path.join(monitor_dir, \"eval_monitor.csv\")\n",
    "eval_env = Monitor(eval_env, filename=eval_monitor_filename)\n",
    "\n",
    "eval_env_parallel_kwargs = {**eval_env_kwargs, \"verbose\": 0, \"print_file_path\": None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future TODOs:\n",
    "\n",
    "- Maybe vectorize env to run N instances in parallel and make tranining faster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume Training:\n",
    "\n",
    "If generally want to resume training of a model:\n",
    "\n",
    "- choose manually the exp_num so it will load the experiment dir you want to resume.\n",
    "- Change the model = ... to load your model.\n",
    "- Observe the model.num_timesteps and define steps as the steps remaining.\n",
    "\n",
    "If the training cell raises an exception and you want to continue training:\n",
    "\n",
    "- Uncomment the cell below and execute it.\n",
    "- comment the lines that redefine the model: model = sb3.PPO... cause it will reset timesteps and model params.\n",
    "- Observe from the cell below the total number of steps the model has already trained. Adjust the `steps` variable to reflect the remaining number of steps you wish to train. Keep all other variables like save freq the same, the model will\n",
    "- Execute the remaining cells below to continue training.\n",
    "- Comment again what you uncomment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rich.console import Console\n",
    "\n",
    "# print(f\"Model trained totally: {model.num_timesteps} steps.\")\n",
    "# console = Console()\n",
    "# console.live = None  # Reset any existing live display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose new or existing model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "# model = stable_baselines3.PPO.load(\n",
    "#     \"./results/crafter/with_skills/exp44/checkpoints/checkpoint_4000_steps.zip\",\n",
    "#     env=env,\n",
    "# )\n",
    "# print(f\"Model trained totally: {model.num_timesteps} steps.\")\n",
    "\n",
    "# Model setup - comment this line if training stopped and want to resume. Otherwise will restart model timesteps and params\n",
    "model = stable_baselines3.PPO(\n",
    "    \"CnnPolicy\",\n",
    "    env,\n",
    "    verbose=1,\n",
    "    tensorboard_log=logs_dir,  # Log for TensorBoard\n",
    ")\n",
    "# To save a non-trained agent uncomment:\n",
    "# model.save(os.path.join(exp_dir, \"final_model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With skills:\n",
    "# - did 3500 steps in 2 hours, so around 1750 steps per hour.\n",
    "\n",
    "# Choose one option from each:\n",
    "\n",
    "# steps = 1e6\n",
    "# steps = 100_000\n",
    "# steps = 10_000  # 10240\n",
    "# steps = 8_000  # will train around 8192 steps\n",
    "# steps = 5_000\n",
    "steps = 4_000\n",
    "# steps = 2014  # minimum. If less it will train around 2014\n",
    "# steps = 2000\n",
    "\n",
    "# save_freq = 50  # save every 50 steps cause LLM API might cause troubles.\n",
    "save_freq = 2_000\n",
    "\n",
    "# save_freq = 50_000\n",
    "\n",
    "# eval_freq = save_freq * 10  # evaluate every 500 cause cost of evaluation is high.\n",
    "# eval_freq = 200\n",
    "eval_freq = save_freq\n",
    "\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=save_freq,\n",
    "    save_path=checkpoints_dir,\n",
    "    name_prefix=\"checkpoint\",\n",
    "    verbose=2,\n",
    ")\n",
    "n_eval_episodes = 5  # 5 is the default value, each episode takes ~170 steps for beginner agent so take it into account.\n",
    "\n",
    "# Not needed, we can evaluate the checkpoints ourself.\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    n_eval_episodes=n_eval_episodes,\n",
    "    best_model_save_path=os.path.join(exp_dir, \"best_model\"),\n",
    "    log_path=eval_dir,\n",
    "    eval_freq=eval_freq,  # Evaluate every 10K steps\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Model trained before current training for {model.num_timesteps} timesteps\")\n",
    "\n",
    "\n",
    "callback = CallbackList([checkpoint_callback, eval_callback])\n",
    "\n",
    "# Save pickle of training args in exp_dir/run_args.pkl:\n",
    "save_args = {\n",
    "    # \"steps\": steps, # dont save steps, the model.timesteps aggr steps from all trainings.\n",
    "    \"save_freq\": save_freq,\n",
    "    \"eval_freq\": eval_freq,\n",
    "    \"n_eval_episodes\": n_eval_episodes,\n",
    "    \"use_skills\": use_skills,\n",
    "    # \"env_kwargs\": env_kwargs, # cause errors\n",
    "    # \"eval_env_kwargs\": eval_env_kwargs,\n",
    "    \"print_file_path\": print_file_path,\n",
    "    \"eval_print_file_path\": eval_print_file_path,\n",
    "    \"verbose\": verbose,\n",
    "    \"exp_dir\": exp_dir,\n",
    "    \"checkpoints_dir\": checkpoints_dir,\n",
    "    \"logs_dir\": logs_dir,\n",
    "    \"eval_dir\": eval_dir,\n",
    "    \"monitor_dir\": monitor_dir,\n",
    "    \"figures_dir\": figures_dir,\n",
    "    \"train_monitor_filename\": train_monitor_filename,\n",
    "    \"eval_monitor_filename\": eval_monitor_filename,\n",
    "    # \"callbacks\": callback,  # cause errors.\n",
    "}\n",
    "if use_skills:\n",
    "    # save_args[\"llm_skills_args\"] = llm_skills_args # errors\n",
    "    save_args[\"mask_actions_indices\"] = mask_actions_indices\n",
    "    save_args[\"skills\"] = skills\n",
    "    # save_args[\"option_policy\"] = option_policy # errors\n",
    "    save_args[\"option_policyClassname\"] = option_policy.__class__.__name__\n",
    "    save_args[\"num_steps_pass_llm\"] = num_steps_pass_llm\n",
    "    save_args[\"default_action_index\"] = default_action_index\n",
    "\n",
    "pickle.dump(save_args, open(os.path.join(exp_dir, \"run_args.pkl\"), \"wb\"))\n",
    "\n",
    "# Training\n",
    "model.learn(\n",
    "    total_timesteps=steps,\n",
    "    log_interval=1,  # It prints performance but not check in indep env, only during training. how many timesteps before logging to console. 1 say each episode.\n",
    "    reset_num_timesteps=False,  # It's for lr schedules and other time-dep things like viz and logging purposes. Can run next times with False and will aggregate. performance acroos all runs think.\n",
    "    progress_bar=True,\n",
    "    callback=callback,\n",
    ")\n",
    "model.save(os.path.join(exp_dir, \"final_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Model trained totally include all trainings for {model.num_timesteps} timesteps\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots:\n",
    "\n",
    "- The following cells generate figures, which can be executed manually using the cell below without requiring the training process to finish. They only need the evaluation callback to run at least once and the monitor logs to be written.\n",
    "- Use the cell below to manually specify the output directory for saving figures and the input files:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# For manual plotting define exp_dir:\n",
    "# exp_dir = \"./results/crafter/with_skills/exp44\"\n",
    "# monitor_dir = os.path.join(exp_dir, \"monitor\")\n",
    "# figures_dir = os.path.join(exp_dir, \"figures\")\n",
    "# eval_dir = os.path.join(exp_dir, \"eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = np.load(os.path.join(eval_dir, \"evaluations.npz\"))\n",
    "\n",
    "print(f\"Eval timesteps: {evals['timesteps']}\")\n",
    "print(\n",
    "    f\"Eval ep_lengths: {evals['ep_lengths']}\"\n",
    ")  # grow with time if agent learn to survive longer. cols as number of episodes per evaluation. row for each evaluation.\n",
    "results = evals[\"results\"]\n",
    "print(\n",
    "    f\"Eval results: {results}\"\n",
    ")  # rows are the evaluations. ith col is the ith episode cumulative return in each evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seaborn:\n",
    "import seaborn as sns\n",
    "\n",
    "# This is typically a 2D array (num_eval_steps, num_episodes_per_eval)\n",
    "ep_lengths = evals[\"ep_lengths\"]  # Episode lengths\n",
    "timesteps = evals[\"timesteps\"]\n",
    "# Compute mean and std deviation of results per evaluation step.\n",
    "# First dim selects the eval index, we want to get mean inside each eval so we take mean on axis=1 which are the cols\n",
    "mean_rewards = results.mean(axis=1)\n",
    "std_rewards = results.std(axis=1)\n",
    "\n",
    "# Plot mean reward over timesteps\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(timesteps, mean_rewards, label=\"Mean Reward\", color=\"blue\")\n",
    "plt.fill_between(\n",
    "    timesteps,\n",
    "    mean_rewards - std_rewards,\n",
    "    mean_rewards + std_rewards,\n",
    "    alpha=0.3,\n",
    "    color=\"blue\",\n",
    ")\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Eval Mean Reward\")\n",
    "plt.title(\"Evaluation Mean Reward Over Time\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(\n",
    "    os.path.join(figures_dir, \"eval_mean_reward.png\"), dpi=300, bbox_inches=\"tight\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Display the evaluation data in a tabular format\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Timesteps\": timesteps,\n",
    "        \"Mean Reward\": mean_rewards,\n",
    "        \"Std Reward\": std_rewards,\n",
    "        \"Mean Episode Length\": ep_lengths.mean(axis=1),\n",
    "    }\n",
    ")\n",
    "print(df)  # show the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_monitor_file = os.path.join(monitor_dir, \"train_monitor.csv\")\n",
    "eval_monitor_file = os.path.join(monitor_dir, \"eval_monitor.csv\")\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(train_monitor_file, comment=\"#\")\n",
    "eval_df = pd.read_csv(eval_monitor_file, comment=\"#\")\n",
    "\n",
    "\n",
    "# Ensure x-ticks are only integers\n",
    "train_episodes = np.arange(len(train_df))\n",
    "eval_episodes = np.arange(len(eval_df))\n",
    "\n",
    "# Plot Episode Rewards Over Time\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_df[\"r\"], label=\"Training Reward\", color=\"blue\", alpha=0.7)\n",
    "plt.plot(eval_df[\"r\"], label=\"Evaluation Reward\", color=\"red\", alpha=0.7)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Episode Rewards Over Time\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "# Set x-axis ticks to integers only\n",
    "plt.xticks(np.arange(0, max(len(train_df), len(eval_df)), step=1))\n",
    "\n",
    "plt.savefig(os.path.join(figures_dir, \"monitor_reward_plot.png\"))\n",
    "plt.show()\n",
    "\n",
    "# Plot Episode Lengths Over Time\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_df[\"l\"], label=\"Training Episode Length\", color=\"blue\", alpha=0.7)\n",
    "plt.plot(eval_df[\"l\"], label=\"Evaluation Episode Length\", color=\"red\", alpha=0.7)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Episode Length\")\n",
    "plt.title(\"Episode Lengths Over Time\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Set x-axis ticks to integers only\n",
    "plt.xticks(np.arange(0, max(len(train_df), len(eval_df)), step=1))\n",
    "\n",
    "plt.savefig(os.path.join(figures_dir, \"monitor_episode_length_plot.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard Instructions:\n",
    "\n",
    "- To see 1 line per experiment, set unsmoothing in settings to 0, it removes the smoothed curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the root dir to show all experiments underneeth it somewhere in the hirarchy:\n",
    "tensorboard_dir = env_results_dir\n",
    "# make it abs path:\n",
    "tensorboard_dir = os.path.abspath(tensorboard_dir)\n",
    "print(f\"tensorboard_dir: {tensorboard_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open browser with the tensorboard logs:\n",
    "!tensorboard --logdir {tensorboard_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard with manually choosing the logdir of the experiment:\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval_episodes(model, env, num_episodes=5):\n",
    "#     rewards = []\n",
    "#     for ep_idx in range(num_episodes):\n",
    "#         print(\"Evaluating episode:\", ep_idx)\n",
    "#         obs, info = env.reset()\n",
    "#         done = False\n",
    "#         episode_reward = 0\n",
    "#         step_idx = 0\n",
    "#         while not done:\n",
    "#             print(f\"Step: {step_idx} of episode {ep_idx}\")\n",
    "#             action, _ = model.predict(obs, deterministic=True)\n",
    "#             obs, reward, done, truncated, info = env.step(action)\n",
    "#             episode_reward += reward\n",
    "#             step_idx += 1\n",
    "#         rewards.append(episode_reward)\n",
    "#     return rewards\n",
    "\n",
    "\n",
    "# def eval_episodes_parallel(\n",
    "#     model, env_ctor, num_episodes_eval_in_parallel=5, eval_env_parallel_kwargs=None\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Evaluates the model on multiple episodes in parallel.\n",
    "\n",
    "#     :param model: Trained RL model\n",
    "#     :param env_ctor: Constructor for the environment (e.g., Dog, not an instance)\n",
    "#     :param num_episodes_eval_in_parallel: Number of evaluation episodes\n",
    "#     :param eval_env_parallel_kwargs: Additional kwargs for creating evaluation environments\n",
    "#     :return: List of episode rewards\n",
    "#     \"\"\"\n",
    "#     if eval_env_parallel_kwargs is None:\n",
    "#         eval_env_parallel_kwargs = {}\n",
    "\n",
    "#     # Create parallel environments\n",
    "#     def make_env():\n",
    "#         return env_ctor(**eval_env_parallel_kwargs)\n",
    "\n",
    "#     envs = SubprocVecEnv([make_env for _ in range(num_episodes_eval_in_parallel)])\n",
    "\n",
    "#     obs, info = envs.reset()\n",
    "#     dones = np.array([False] * num_episodes_eval_in_parallel)\n",
    "#     episode_rewards = np.zeros(num_episodes_eval_in_parallel)\n",
    "\n",
    "#     while not np.all(dones):\n",
    "#         actions, _ = model.predict(obs, deterministic=True)\n",
    "#         obs, reward, done, truncated, info = envs.step(actions)\n",
    "#         episode_rewards += rewards * (~dones)  # Only add reward for ongoing episodes\n",
    "\n",
    "#     envs.close()\n",
    "#     return episode_rewards.tolist()\n",
    "\n",
    "\n",
    "# # model = stable_baselines3.PPO.load(os.path.join(exp_dir, \"final_model\"))\n",
    "\n",
    "# # num_episodes_eval_in_parallel = 5\n",
    "# # episode_rewards = eval_episodes_parallel(\n",
    "# #     model, CrafterGymnasium, num_episodes_eval_in_parallel, eval_env_parallel_kwargs\n",
    "# # )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UbuntuDec24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
